# Expanded Model Configuration for HMER-Ink
# Following progressive model expansion strategy from a smaller model

# Data settings
data:
  data_dir: "data"
  train_dirs: ["train"]
  valid_dir: "valid"
  test_dir: "test"
  max_seq_length: 512  # Increased from 384
  max_token_length: 256  # Increased from 192
  use_synthetic: false
  normalization:
    x_range: [-1, 1]
    y_range: [-1, 1]
    time_range: [0, 1]
  augmentation:
    enabled: true
    scale_range: [0.8, 1.2]
    rotation_range: [-10, 10]
    translation_range: [-0.1, 0.1]
    stroke_dropout_prob: 0.05
    jitter_scale: 0.01
    stroke_thickness_range: [1.0, 2.0]

# Model architecture (expanded)
model:
  name: "transformer_encoder_decoder"
  encoder:
    type: "transformer"
    input_dim: 4  # x, y, t, pen_state
    embedding_dim: 384  # Increased from 256
    num_layers: 5  # Increased from 3
    num_heads: 12  # Increased from 8
    dropout: 0.15  # Slightly increased dropout for larger model
    position_encoding: "sinusoidal"
  decoder:
    type: "transformer"
    embedding_dim: 384  # Increased from 256
    num_layers: 5  # Increased from 3
    num_heads: 12  # Increased from 8
    dropout: 0.15  # Slightly increased dropout
    max_length: 256  # Increased from 192

# Training settings
training:
  batch_size: 48  # Reduced from 64 due to larger model
  learning_rate: 0.0003  # Reduced from 0.0005 for fine-tuning
  weight_decay: 0.0001
  num_epochs: 50
  early_stopping_patience: 10  # Increased patience for fine-tuning
  lr_scheduler:
    type: "reduce_on_plateau"
    factor: 0.7  # More gradual reduction (was 0.5)
    patience: 3  # Increased from 2
    threshold: 0.001
  optimizer: "adamw"
  use_amp: true
  device: "mps"
  num_workers: 4
  gradient_accumulation_steps: 10  # Increased from 8 to compensate for smaller batch
  save_every_n_epochs: 1
  validate_every_n_steps: 2500  # Validate more frequently
  clip_grad_norm: 1.0

# MPS specific optimizations
mps_configuration:
  enable_mps_fallback: true
  verbose: false
  high_watermark_ratio: 0.0
  prefer_channels_last: true
  enable_early_graph_capture: true
  separate_device_alloc: true
  use_system_allocator: true

# Evaluation settings
evaluation:
  batch_size: 24  # Reduced from 32 due to larger model
  device: "mps"
  metrics: ["edit_distance", "character_error_rate", "expression_recognition_rate", "symbol_accuracy", "token_error_rate"]
  beam_size: 4
  num_error_examples: 10

# Outputs
output:
  checkpoint_dir: "outputs/checkpoints"
  log_dir: "outputs/logs"
  use_wandb: true
  project_name: "hmer-ink-expanded"  # New project name for the expanded model
  tensorboard: false
  save_best_k: 2
  monitor_metric: "expression_recognition_rate"
  monitor_mode: "max"  # Higher is better for ERR
  record_metrics: true  # Automatically save metrics to JSON for monitoring
  metrics_dir: "outputs/training_metrics"
# Enhanced Fast Training Configuration for HMER-Ink
# Balanced for better performance without excessive compute cost (<25% additional compute vs fast.yaml)

# Data settings
data:
  data_dir: "data"
  train_dirs: ["train", "symbols"]  # Add symbols dataset but skip synthetic
  valid_dir: "valid"
  test_dir: "test"
  max_seq_length: 384  # Same as fast config
  max_token_length: 224  # Slightly increased for better sequence modeling
  use_synthetic: false  # Still skip synthetic data for faster training
  normalization:
    x_range: [-1, 1]
    y_range: [-1, 1]
    time_range: [0, 1]
  augmentation:
    enabled: true
    scale_range: [0.75, 1.25]  # Slightly more aggressive scaling
    rotation_range: [-15, 15]  # Increased rotation range
    translation_range: [-0.125, 0.125]  # Increased translation
    stroke_dropout_prob: 0.05
    jitter_scale: 0.01
    stroke_thickness_range: [1.0, 2.25]

# Model architecture (balanced)
model:
  name: "transformer_encoder_decoder"
  encoder:
    type: "transformer"
    input_dim: 4  # x, y, t, pen_state
    embedding_dim: 320  # Increased from 256 for better representation
    num_layers: 4  # One more layer than fast config
    num_heads: 8
    dropout: 0.15  # Slightly increased for better generalization
    position_encoding: "sinusoidal"
  decoder:
    type: "transformer"
    embedding_dim: 320  # Matching encoder
    num_layers: 4  # One more layer than fast config
    num_heads: 8
    dropout: 0.15
    max_length: 224  # Match token length

# Training settings - balanced optimization
training:
  batch_size: 48  # Slightly reduced from fast config
  learning_rate: 0.0004  # Adjusted learning rate
  weight_decay: 0.0001
  num_epochs: 80  # More epochs than fast config
  early_stopping_patience: 10  # More patience for better convergence
  lr_scheduler:
    type: "reduce_on_plateau"
    factor: 0.5
    patience: 3  # Slightly more patience than fast config
    threshold: 0.001
  optimizer: "adamw"
  use_amp: true  # Mixed precision for faster training
  device: "mps"  # For Apple Silicon
  num_workers: 4
  gradient_accumulation_steps: 8 
  save_every_n_epochs: 2
  validate_every_n_steps: 1500  # Validate more often than fast config
  clip_grad_norm: 1.0

# MPS specific optimizations
mps_configuration:
  enable_mps_fallback: true
  verbose: false  # Reduce logging noise
  high_watermark_ratio: 0.0
  prefer_channels_last: true
  enable_early_graph_capture: true
  separate_device_alloc: true
  use_system_allocator: true

# Evaluation settings
evaluation:
  batch_size: 32
  device: "mps"
  metrics: ["edit_distance", "character_error_rate", "expression_recognition_rate", "symbol_accuracy", "token_error_rate"]
  beam_size: 4  # Increased beam size for better accuracy
  num_error_examples: 15

# Outputs
output:
  model_dir: "outputs/models"  # Base directory for all model files
  checkpoint_dir: "checkpoints"  # Relative to model_dir
  log_dir: "logs"  # Relative to model_dir
  metrics_dir: "metrics"  # Relative to model_dir
  
  use_wandb: true  # Enable Weights & Biases for tracking progress
  project_name: "hmer-ink-fast-enhanced"  # Distinct project name
  tensorboard: false  # Disable tensorboard
  save_best_k: 2  # Save top 2 models
  monitor_metric: "expression_recognition_rate"
  monitor_mode: "max"  # Higher is better for ERR
  record_metrics: true  # Automatically save metrics to JSON for monitoring
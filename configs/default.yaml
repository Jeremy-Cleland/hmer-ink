# Optimized Training Configuration for HMER-Ink
# Balanced for reliable convergence with good performance

# Data settings
data:
  data_dir: "data"
  train_dirs: ["train", "symbols"] # Using only train and symbols data (no synthetic)
  valid_dir: "valid"
  test_dir: "test"
  max_seq_length: 384
  max_token_length: 224
  use_synthetic: false # Set to false to avoid using synthetic data which increases training time
  curriculum:
    enabled: false # Enable curriculum learning (easy to hard)
    metric: "token_length" # Options: token_length, seq_length
    epochs_to_full_difficulty: 15 # Number of epochs until full difficulty range
  vocab_random_seed: 42
  dataloader_seed: 42 # Fixed seed for DataLoader to ensure consistent batching
  vocab_file: "vocab.json"
  use_shared_vocab: true # Use shared vocabulary for consistent training/resuming
  normalization:
    x_range: [-1, 1]
    y_range: [-1, 1]
    time_range: [0, 1]
    preserve_aspect_ratio: true  # Prevent distortion by maintaining original proportions
  augmentation:
    enabled: true
    scale_range: [0.9, 1.1] # Very conservative scaling to prevent distortion
    rotation_range: [-10, 10] # Rotation angles in degrees
    rotation_probability: 0.7 # Probability of applying rotation
    translation_range: [-0.05, 0.05] # Small translations
    stroke_dropout_prob: 0.03 # Low probability of stroke dropout
    max_dropout_ratio: 0.2 # Maximum ratio of strokes that can be dropped
    jitter_scale: 0.005 # Minimal jitter to retain legibility
    jitter_probability: 0.7 # Probability of applying jitter to each point
    stroke_thickness_range: [1.0, 2.0] # Original thickness values

# Model architecture - optimal for math expression recognition
model:
  name: "transformer_encoder_decoder"
  encoder:
    type: "transformer"
    input_dim: 4 # x, y, t, pen_state
    embedding_dim: 320 # Keep capacity for math symbol relationships
    num_layers: 4
    num_heads: 8
    dropout: 0.1 # Lower dropout for better training signal
    position_encoding: "sinusoidal"
    use_bbox_data: true # Enable bounding box data utilization
  decoder:
    type: "transformer"
    embedding_dim: 320
    num_layers: 4
    num_heads: 8
    dropout: 0.1
    max_length: 224

# Training settings - optimized for reliable convergence
training:
  batch_size: 32
  learning_rate: 0.0003 # Higher starting LR to help model learn faster
  weight_decay: 0.00007
  num_epochs: 100
  early_stopping_patience: 15
  lr_scheduler:
    type: "one_cycle" # Use one cycle for better convergence
    max_lr: 0.001 # Higher peak learning rate to accelerate early learning
    pct_start: 0.1 # Reach peak LR earlier in training
    div_factor: 3.0 # Initial LR = max_lr/div_factor
    final_div_factor: 10.0 # Final LR = initial_lr/final_div_factor
  optimizer: "adamw"
  use_amp: true
  device: "mps"
  num_workers: 4
  gradient_accumulation_steps: 6
  save_every_n_epochs: 2
  validate_every_n_steps: 1000
  clip_grad_norm: 1.0
  label_smoothing: 0.1 # Add label smoothing for regularization

# MPS specific optimizations
mps_configuration:
  enable_mps_fallback: true
  verbose: false
  high_watermark_ratio: 0.0
  prefer_channels_last: true
  enable_early_graph_capture: true
  separate_device_alloc: true
  use_system_allocator: true

# Evaluation settings
evaluation:
  batch_size: 32
  device: "mps"
  metrics:
    [
      "edit_distance",
      "normalized_edit_distance",
      "exact_match",
      "expression_recognition_rate",
      "symbol_accuracy",
    ]
  beam_size: 3
  val_max_samples: 100 # More validation samples
  error_analysis_batches: 2

# Outputs
output:
  model_dir: "outputs/models"
  checkpoint_dir: "checkpoints"
  log_dir: "logs"
  metrics_dir: "metrics"

  use_wandb: true
  project_name: "hmer-ink-m4-max"
  tensorboard: false
  save_best_k: 3
  monitor_metric: "val_loss" # Monitor validation loss for early stopping
  monitor_mode: "min"
  record_metrics: true
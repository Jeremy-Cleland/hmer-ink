# HMER-Ink Optimized Configuration

# Data settings
data:
  data_dir: "data"
  train_dirs: ["train", "synthetic", "symbols"]  # Use all available training data
  valid_dir: "valid"
  test_dir: "test"
  max_seq_length: 1024  # Increased from 512 for better handling of complex expressions
  max_token_length: 256  # Increased to handle longer LaTeX expressions
  use_synthetic: true  # Leverage synthetic data
  normalization:
    x_range: [-1, 1]  # Normalize x coordinates to this range
    y_range: [-1, 1]  # Normalize y coordinates to this range
    time_range: [0, 1]  # Normalize time to this range
  augmentation:
    enabled: true
    scale_range: [0.7, 1.3]  # More aggressive scaling
    rotation_range: [-20, 20]  # Wider rotation range for better robustness
    translation_range: [-0.15, 0.15]  # Increased translation range
    stroke_dropout_prob: 0.05
    jitter_scale: 0.01  # Add small jitter for noise robustness
    stroke_thickness_range: [1.0, 2.5]

# Model architecture
model:
  name: "transformer_encoder_decoder"
  encoder:
    type: "transformer"  # Options: transformer, bilstm, cnn
    input_dim: 4  # x, y, t, pen_state
    embedding_dim: 384  # Increased dimension for better representation
    num_layers: 6  # Deeper encoder
    num_heads: 8
    dropout: 0.2  # Increased dropout for better generalization
    position_encoding: "sinusoidal"
  decoder:
    type: "transformer"  # Options: transformer, lstm
    embedding_dim: 384
    num_layers: 6  # Deeper decoder
    num_heads: 8
    dropout: 0.2
    max_length: 256

# Training settings
training:
  batch_size: 64  # Adjust based on your GPU/MPS memory
  learning_rate: 0.0003  # Slightly lower learning rate for stability
  weight_decay: 0.0001
  num_epochs: 100  # Train for longer
  early_stopping_patience: 10  # More patience
  lr_scheduler:
    type: "reduce_on_plateau"
    factor: 0.5
    patience: 3
    threshold: 0.001
  optimizer: "adamw"  # Better weight decay handling
  use_amp: true  # Automatic Mixed Precision for faster training
  device: "mps"  # For Apple Silicon
  num_workers: 4
  gradient_accumulation_steps: 2  # Accumulate gradients for larger effective batch size
  save_every_n_epochs: 1
  validate_every_n_steps: 1000
  clip_grad_norm: 1.0  # Add gradient clipping for stability

# Evaluation settings
evaluation:
  batch_size: 32
  metrics: ["edit_distance", "normalized_edit_distance", "exact_match", "expression_recognition_rate", "symbol_accuracy"]
  beam_size: 5  # Increased beam size for better results

# Outputs
output:
  checkpoint_dir: "outputs/checkpoints"
  log_dir: "outputs/logs"
  use_wandb: true
  project_name: "hmer-ink"
  tensorboard: true
  save_best_k: 3  # Save the k best models
  monitor_metric: "expression_recognition_rate"  # Metric to monitor for best model
  monitor_mode: "max"  # Higher is better for ERR
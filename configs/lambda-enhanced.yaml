# Enhanced Training Configuration for HMER-Ink on Lambda Labs
# Optimized for maximum performance on powerful GPU instances

# Data settings
data:
  data_dir: "data"
  train_dirs: ["train", "symbols", "synthetic"] # Include synthetic data with bounding boxes
  valid_dir: "valid"
  test_dir: "test"
  max_seq_length: 512  # Increased from 384 to handle longer sequences
  max_token_length: 256  # Increased from 224 for more complex expressions
  use_synthetic: true # Enable synthetic data with bounding boxes
  vocab_random_seed: 42
  dataloader_seed: 42 # Fixed seed for DataLoader to ensure consistent batching
  vocab_file: "vocab.json"
  use_shared_vocab: true # Use shared vocabulary for consistent training/resuming
  normalization:
    x_range: [-1, 1]
    y_range: [-1, 1]
    time_range: [0, 1]
    preserve_aspect_ratio: true  # Prevent distortion by maintaining original proportions
  augmentation:
    enabled: true
    scale_range: [0.85, 1.15] # Slightly more aggressive scaling for better generalization
    rotation_range: [-15, 15] # Increased rotation range
    rotation_probability: 0.7
    translation_range: [-0.08, 0.08] # Increased translation range
    stroke_dropout_prob: 0.05 # Slightly increased for better robustness
    max_dropout_ratio: 0.25
    jitter_scale: 0.008 # Increased jitter for better generalization
    jitter_probability: 0.8
    stroke_thickness_range: [1.0, 2.5] # Increased upper range

# Model architecture - enhanced for GPU performance
model:
  name: "transformer_encoder_decoder"
  encoder:
    type: "transformer"
    input_dim: 4 # x, y, t, pen_state
    embedding_dim: 512  # Increased from 320 for better representation capacity
    num_layers: 6      # Increased from 4 for deeper encoding
    num_heads: 8
    dropout: 0.1
    position_encoding: "sinusoidal"
    use_bbox_data: true # Enable bounding box data utilization
    feed_forward_dim: 2048  # Added explicit feed-forward dimension
  decoder:
    type: "transformer"
    embedding_dim: 512  # Matching encoder size
    num_layers: 6      # Increased from 4 for deeper decoding
    num_heads: 8
    dropout: 0.1
    max_length: 256    # Matching max_token_length
    feed_forward_dim: 2048  # Added explicit feed-forward dimension

# Training settings - optimized for Lambda Labs GPU
training:
  batch_size: 64  # Balanced for larger model size
  learning_rate: 0.0003
  weight_decay: 0.00005  # Slightly reduced for larger model
  num_epochs: 150  # Increased to allow more training time for larger model
  early_stopping_patience: 20  # Increased for more opportunity to converge
  lr_scheduler:
    type: "one_cycle"
    max_lr: 0.001
    pct_start: 0.1
    div_factor: 3.0
    final_div_factor: 10.0
  optimizer: "adamw"
  use_amp: true  # Use mixed precision for faster training
  device: "cuda"  # Use CUDA for Lambda Labs GPUs
  num_workers: 12  # Increased for faster data loading
  gradient_accumulation_steps: 2  # Accumulate gradients for larger effective batch
  save_every_n_epochs: 1  # Save more frequently
  validate_every_n_steps: 500  # Validate more frequently
  clip_grad_norm: 1.0
  label_smoothing: 0.1
  warmup_steps: 1000  # Added warmup steps for stability with larger model

# Evaluation settings
evaluation:
  batch_size: 64  # Balanced for larger model
  device: "cuda"
  metrics:
    [
      "edit_distance",
      "normalized_edit_distance",
      "exact_match",
      "expression_recognition_rate",
      "symbol_accuracy",
    ]
  beam_size: 5  # Increased from 3 for better generation
  val_max_samples: 200  # Increased validation sample size
  error_analysis_batches: 3  # More batches for better error analysis

# Outputs
output:
  model_dir: "outputs/models"
  checkpoint_dir: "checkpoints"
  log_dir: "logs"
  metrics_dir: "metrics"

  use_wandb: true
  project_name: "hmer-ink-lambda-enhanced"
  tensorboard: true  # Enable tensorboard for additional monitoring
  save_best_k: 5  # Keep more checkpoints
  monitor_metric: "val_loss"
  monitor_mode: "min"
  record_metrics: true 